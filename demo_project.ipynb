{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e14a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c54fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Adjust this to your folder path\n",
    "SOPS_FOLDER = \"./sops_json\"  # folder containing multiple .json SOP files\n",
    "PERSIST_DIR = \"./chroma_sops\" # where Chroma DB will store data\n",
    "COLLECTION_NAME = \"sops\"\n",
    "\n",
    "# Minimal set of fields expected in each SOP JSON\n",
    "REQUIRED_FIELDS = {\n",
    "    \"sop_id\", \"db_type\", \"error_category\", \"problem_statement\",\n",
    "    \"symptoms\", \"resolution_summary\", \"auto_fix_supported\",\n",
    "    \"requires_human_approval\", \"risk_level\", \"tags\"\n",
    "}\n",
    "\n",
    "def load_sops_from_folder(folder: str) -> List[Dict[str, Any]]:\n",
    "    sops = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".json\"):\n",
    "            path = os.path.join(folder, file)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            missing = REQUIRED_FIELDS - set(data.keys())\n",
    "            if missing:\n",
    "                raise ValueError(f\"{file} missing fields: {missing}\")\n",
    "            sops.append(data)\n",
    "    if not sops:\n",
    "        raise FileNotFoundError(f\"No JSON files found in {folder}\")\n",
    "    print(f\"Loaded {len(sops)} SOPs from {folder}\")\n",
    "    return sops\n",
    "\n",
    "def sop_to_text(sop: Dict[str, Any]) -> str:\n",
    "    symptoms_str = \"\\n\".join(f\"- {s}\" for s in sop.get(\"symptoms\", []))\n",
    "    tags_str = \", \".join(sop.get(\"tags\", []))\n",
    "    text = (\n",
    "        f\"SOP ID: {sop['sop_id']}\\n\"\n",
    "        f\"DB Type: {sop['db_type']}\\n\"\n",
    "        f\"Error Category: {sop['error_category']}\\n\"\n",
    "        f\"Risk Level: {sop['risk_level']}\\n\"\n",
    "        f\"Auto Fix Supported: {sop['auto_fix_supported']}\\n\"\n",
    "        f\"Requires Human Approval: {sop['requires_human_approval']}\\n\\n\"\n",
    "        f\"Problem Statement:\\n{sop['problem_statement']}\\n\\n\"\n",
    "        f\"Symptoms:\\n{symptoms_str}\\n\\n\"\n",
    "        f\"Resolution Summary:\\n{sop['resolution_summary']}\\n\\n\"\n",
    "        f\"Tags: {tags_str}\\n\"\n",
    "    )\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 150) -> List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3421ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Fast, lightweight local embedding model\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    # normalize_embeddings=True -> cosine similarity works well\n",
    "    return embedder.encode(texts, show_progress_bar=False, normalize_embeddings=True).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87783de5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_chroma_collection(persist_dir: str = PERSIST_DIR, collection_name: str = COLLECTION_NAME):\n",
    "    client = chromadb.PersistentClient(path=persist_dir, settings=Settings(allow_reset=True))\n",
    "    try:\n",
    "        collection = client.get_collection(collection_name)\n",
    "    except Exception:\n",
    "        collection = client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # cosine for normalized embeddings\n",
    "        )\n",
    "    return collection\n",
    "\n",
    "collection = get_chroma_collection()\n",
    "collection\n",
    "``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb76861",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def upsert_sop_chunks(collection, sop: Dict[str, Any], chunk_texts: List[str]):\n",
    "    base_meta = {\n",
    "        \"sop_id\": sop[\"sop_id\"],\n",
    "        \"db_type\": sop[\"db_type\"],\n",
    "        \"error_category\": sop[\"error_category\"],\n",
    "        \"risk_level\": sop[\"risk_level\"],\n",
    "        \"auto_fix_supported\": sop[\"auto_fix_supported\"],\n",
    "        \"requires_human_approval\": sop[\"requires_human_approval\"],\n",
    "        \"tags\": sop.get(\"tags\", []),\n",
    "    }\n",
    "    ids, docs, metas = [], [], []\n",
    "    for idx, ct in enumerate(chunk_texts):\n",
    "        cid = f\"{sop['sop_id']}::chunk::{idx}\"\n",
    "        ids.append(cid)\n",
    "        docs.append(ct)\n",
    "        meta = dict(base_meta)\n",
    "        meta[\"chunk_index\"] = idx\n",
    "        metas.append(meta)\n",
    "\n",
    "    embeddings = embed_texts(docs)\n",
    "    collection.upsert(ids=ids, embeddings=embeddings, documents=docs, metadatas=metas)\n",
    "\n",
    "# Load, format, chunk, embed, upsert\n",
    "sops = load_sops_from_folder(SOPS_FOLDER)\n",
    "for sop in sops:\n",
    "    text = sop_to_text(sop)\n",
    "    chunks = chunk_text(text, chunk_size=1000, overlap=150)\n",
    "    upsert_sop_chunks(collection, sop, chunks)\n",
    "\n",
    "print(\"Upsert complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f65ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional\n",
    "\n",
    "def retrieve(collection, query: str, k: int = 5, where: Optional[Dict[str, Any]] = None):\n",
    "    q_emb = embed_texts([query])[0]\n",
    "    results = collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=k,\n",
    "        where=where or {}\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example query\n",
    "query = \"Rubrik backup failed because SQL Server transaction log chain is broken.\"\n",
    "results = retrieve(\n",
    "    collection,\n",
    "    query=query,\n",
    "    k=5,\n",
    "    where={\"db_type\": \"SQL_Server\"}  # try removing or changing filters\n",
    ")\n",
    "\n",
    "for doc, meta, dist in zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]):\n",
    "    print(f\"\\nScore={1 - dist:.4f} | sop_id={meta['sop_id']} | chunk={meta['chunk_index']}\")\n",
    "    print(doc[:500], \"...\" if len(doc) > 500 else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4b053",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
